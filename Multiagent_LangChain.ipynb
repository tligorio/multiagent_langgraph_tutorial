{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Research Assistant with LangGraph\n",
    "#### Authored by Dr. Tiziana Ligorio for *AI Agents - CSCI 395.32* taught at Hunter College of The City University of New York\n",
    "#### Adapted from: [*Large Language Model Agents*, Jerin George Mathew & Jacopo Rossi, Springer 2025](https://link.springer.com/chapter/10.1007/978-3-031-92285-5_8)\n",
    "\n",
    "\n",
    "In this tutorial, we build a research assistant that uses multiple agents to streamline the process of finding and filtering academic research papers. This demonstrates a multi-agent system using the LangGraph framework.\n",
    "\n",
    "The system consists of four specialized agents:\n",
    "\n",
    "1. **Search Agent** â€” Queries Google Scholar and/or arXiv to find academic papers matching the user's query\n",
    "2. **Filter Agent** â€” Evaluates the relevance of retrieved papers and adds relevant ones to the filtered papers list\n",
    "3. **Query Refinement Agent** â€” Refines the search query to improve results when the current query yields insufficient relevant papers\n",
    "4. **Supervisor Agent** â€” Decides whether the workflow should finalize (enough relevant papers found) or continue refining the query\n",
    "\n",
    "## Workflow\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/tligorio/multiagent_langgraph_tutorial/main/images/multiagent.png\" alt=\"Multiagent System - web\" width=\"50%\"/>\n",
    "\n",
    "\n",
    "\n",
    "## Stopping Criteria\n",
    "\n",
    "The Supervisor Agent finalizes the workflow when at least **3 papers** have been identified with a **relevance score â‰¥ 0.7**. Otherwise, the Query Refinement Agent generates an improved query and the search process iterates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installs and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install langgraph langchain langchain-openai arxiv scholarly python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`%%capture` hides the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tizianaligorio/dev/Agents_course/multiagent_langgraph_tutorial/.venv/lib/python3.12/site-packages/scholarly/_scholarly.py:312: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  m = re.search(\"cites=[\\d+,]*\", object[\"citedby_url\"])\n"
     ]
    }
   ],
   "source": [
    "# LangGraph: multi-agent orchestration framework\n",
    "# Provides StateGraph for defining agent workflows with nodes and edges\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# LangChain: core framework for LLM applications\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage # define roles\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# OpenAI integration for LangChain (used with OpenRouter)\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Academic paper search\n",
    "import arxiv  # arXiv API client\n",
    "from scholarly import scholarly  # Google Scholar scraper\n",
    "\n",
    "\n",
    "# Standard library\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OpenRouter** is a unified API that provides access to various LLMs through a single interface. It offers a generous free tier and affordable token usage for minimal cost, making it ideal for learning and experimentation.\n",
    "\n",
    "If you already pay for other LLM providers or prefer to use a different service, you are welcome to adapt the code accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup your API Key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 â€” Get an OpenRouter API key\n",
    "For this demo we will use an LLM via OpenRouter, which requires an API key.\n",
    "\n",
    "1. Go to https://openrouter.ai\n",
    "\n",
    "2. Sign in (or create an account if you don't have one)\n",
    "\n",
    "3. Once logged in, navigate to https://openrouter.ai/settings/keys\n",
    "\n",
    "4. Click Create Key\n",
    "\n",
    "5. Give the key a name, e.g. colab-multiagent_langgraph\n",
    "\n",
    "6. Copy the key immediately (you won't be able to see it again)\n",
    "\n",
    "**Important:\n",
    "Treat this key like a password.\n",
    "Do not share it, paste it into notebooks, or commit it to GitHub.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 â€” Add a secret in Colab (UI)\n",
    "\n",
    "1.  On the left sidebar, click ðŸ”‘ Secrets\n",
    "\n",
    "2.  Add a new secret:\n",
    "\n",
    "\n",
    "*   Name: OPENROUTER_API_KEY\n",
    "*   Value: your actual API key\n",
    "\n",
    "3. Toggle the switch to the left to give notebook access (you should see a checkmark)\n",
    "\n",
    "## If running locally â€” Add a secret in .env\n",
    "\n",
    "1. Create a .env file in the project root:\n",
    "\n",
    "`touch .env`\n",
    "\n",
    "2.  Add the following (replace with your own key):\n",
    "\n",
    "`OPENROUTER_API_KEY=your_openrouter_key_here`. \n",
    "\n",
    "\n",
    "**Important: Never paste API keys into code cells.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the API Key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Colab:\n",
    "Uncomment and run the cell below if you're using Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load API key from Colab Secrets into environment variable if running in Colab\n",
    "# from google.colab import userdata\n",
    "\n",
    "# key = \"OPENROUTER_API_KEY\"\n",
    "# value = userdata.get(key)\n",
    "# assert value is not None, f\"{key} not found in Colab Secrets or access is disabled\"\n",
    "# os.environ[key] = value\n",
    "\n",
    "# print(\"API key successfully loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API key from .env if running locally - see local install instructions in the repo README\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENROUTER_API_KEY present: True\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "print(\"OPENROUTER_API_KEY present:\", bool(os.getenv(\"OPENROUTER_API_KEY\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Define the State Schema\n",
    "\n",
    "In a multi-agent system, the state serves as the shared memory through which agents communicate and coordinate. Each agent reads from and writes to this common structure, enabling them to build on each other's work without direct interaction.  \n",
    "\n",
    "In LangGraph, the **state** is a shared data structure that flows through the graph and gets updated by each node. We define it as a `TypedDict` to specify what fields exist and their types.\n",
    "\n",
    "When a node returns a dictionary, LangGraph **merges** it into the current state:\n",
    "- For regular fields, the returned value **replaces** the existing value\n",
    "- For fields using `Annotated` with a reducer (like `operator.add`), the returned value is **combined** with the existing value\n",
    "\n",
    "This is important for our workflow:\n",
    "- `papers` gets replaced on each search (we only want the current iteration's results)\n",
    "- `filtered_papers` accumulates across iterations (we want to keep all relevant papers found so far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    Shared state that flows through the graph and is updated by each node.\n",
    "    \n",
    "    When a node returns {\"field\": value}, LangGraph merges it into the state:\n",
    "    - Regular fields: new value REPLACES the old value\n",
    "    - Annotated fields with reducer: new value is COMBINED with old value using the reducer\n",
    "    \"\"\"\n",
    "    \n",
    "    # The current search query, replaced when Query Refinement Agent updates it\n",
    "    query: str\n",
    "    \n",
    "    # Raw search results from the current iteration, replaced on each search\n",
    "    # (we only need the latest batch to filter)\n",
    "    papers: List[dict]\n",
    "    \n",
    "    # Papers that passed relevance filtering, ACCUMULATES across iterations\n",
    "    # Using operator.add as reducer means new papers are appended, not replaced\n",
    "    filtered_papers: Annotated[List[dict], operator.add]\n",
    "    \n",
    "    # All evaluations from the Filter Agent (for debugging/inspection)\n",
    "    # Each entry contains: title, score, justification - regardless of whether it passed\n",
    "    #\n",
    "    # We use a custom \"replace\" reducer (lambda old, new: new) instead of operator.add because:\n",
    "    # 1. LangGraph requires Annotated fields for proper state tracking between nodes\n",
    "    # 2. But we only need the CURRENT iteration's evaluations for query refinement context\n",
    "    # 3. Accumulating all evaluations across iterations would waste memory/tokens\n",
    "    # The lambda simply returns the new value, effectively replacing instead of appending\n",
    "    all_evaluations: Annotated[List[dict], lambda old, new: new]\n",
    "    \n",
    "    # Tracks how many search iterations we've done (to prevent excessively long iterations or infinite loops)\n",
    "    iteration: int\n",
    "    \n",
    "    # Decision from the Supervisor Agent: \"end\" or \"refine\"\n",
    "    decision: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a test state that we can pass to agent functions for testing\n",
    "# This lets us test each agent independently before wiring them into the graph\n",
    "\n",
    "test_state: AgentState = {\n",
    "    \"query\": \"multi-agent reinforcement learning\",  # The research topic to search for\n",
    "    \"papers\": [],                                   # Will be populated by search_agent\n",
    "    \"filtered_papers\": [],                          # Will be populated by filter_agent\n",
    "    \"all_evaluations\": [],                          # Will store all LLM evaluations for debugging\n",
    "    \"iteration\": 0,                                 # Starting iteration\n",
    "    \"decision\": \"\"                                  # Will be set by supervisor_agent\n",
    "}\n",
    "\n",
    "print(\"Test state initialized:\")\n",
    "print(f\"  query: '{test_state['query']}'\")\n",
    "print(f\"  papers: {len(test_state['papers'])} items\")\n",
    "print(f\"  filtered_papers: {len(test_state['filtered_papers'])} items\")\n",
    "print(f\"  all_evaluations: {len(test_state['all_evaluations'])} items\")\n",
    "print(f\"  iteration: {test_state['iteration']}\")\n",
    "print(f\"  decision: '{test_state['decision']}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Agent\n",
    "\n",
    "The Search Agent is responsible for querying academic paper databases to find papers matching the user's research query. It uses the arXiv API to search for papers and returns structured metadata for each result.\n",
    "\n",
    "**Input:** Takes the current `query` from the state  \n",
    "**Output:** Returns a list of papers with metadata (title, authors, abstract, URL, publication date)  \n",
    "**Tools:** arXiv API client\n",
    "\n",
    "The agent does not use an LLM â€” it's a straightforward API call that retrieves papers based on keyword matching. The LLM-based reasoning happens in the Filter Agent, which evaluates relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a single arXiv client to reuse across all searches\n",
    "# This ensures proper rate limiting (the client tracks request timestamps internally)\n",
    "arxiv_client = arxiv.Client()\n",
    "\n",
    "def search_agent(state: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Search for academic papers on arXiv based on the current query.\n",
    "    \n",
    "    Args:\n",
    "        state: Current graph state containing 'query'\n",
    "        \n",
    "    Returns:\n",
    "        Updated state with 'papers' list containing search results\n",
    "    \"\"\"\n",
    "    query = state[\"query\"]\n",
    "    max_results = 10\n",
    "    \n",
    "    # Search arXiv using the shared client (handles rate limiting internally)\n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "    \n",
    "    papers = []\n",
    "    for result in arxiv_client.results(search):\n",
    "        paper = {\n",
    "            \"title\": result.title,\n",
    "            \"authors\": [author.name for author in result.authors],\n",
    "            \"abstract\": result.summary,\n",
    "            \"url\": result.entry_id,\n",
    "            \"published\": result.published.strftime(\"%Y-%m-%d\"),\n",
    "            \"source\": \"arxiv\"\n",
    "        }\n",
    "        papers.append(paper)\n",
    "    \n",
    "    print(f\"Search Agent: Found {len(papers)} papers for query '{query}'\")\n",
    "    \n",
    "    return {\"papers\": papers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Agent: Found 10 papers for query 'multi-llm-agent reinforcement learning'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers = search_agent(test_state)[\"papers\"]  \n",
    "len(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-27: ARLBench: Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning\n",
      "2025-06-24: Causal-Paced Deep Reinforcement Learning\n",
      "2018-07-13: Exploring Hierarchy-Aware Inverse Reinforcement Learning\n",
      "2024-01-14: Small LLMs Are Weak Tool Learners: A Multi-LLM Agent\n",
      "2023-01-19: A Tutorial on Meta-Reinforcement Learning\n",
      "2018-09-25: Anderson Acceleration for Reinforcement Learning\n",
      "2024-06-07: Stabilizing Extreme Q-learning by Maclaurin Expansion\n",
      "2019-09-26: MERL: Multi-Head Reinforcement Learning\n",
      "2025-08-09: Context Engineering for Multi-Agent LLM Code Assistants Using Elicit, NotebookLM, ChatGPT, and Claude Code\n",
      "2019-04-20: Compression and Localization in Reinforcement Learning for ATARI Games\n"
     ]
    }
   ],
   "source": [
    "titles = [paper[\"title\"] for paper in papers]                                              \n",
    "dates = [paper[\"published\"] for paper in papers]                                           \n",
    "                                                                                            \n",
    "for title, date in zip(titles, dates):                                                     \n",
    "    print(f\"{date}: {title}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add papers to test_state for testing\n",
    "test_state[\"papers\"] = papers\n",
    "len(test_state[\"papers\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Agent\n",
    "\n",
    "The Filter Agent evaluates the relevance of each paper retrieved by the Search Agent. Unlike the Search Agent, this agent **uses an LLM** to reason about semantic relevance â€” determining whether a paper's content actually addresses the user's research question, not just whether it contains matching keywords.\n",
    "\n",
    "**Input:** Takes `papers` (raw search results) and `query` from the state  \n",
    "**Output:** Returns papers that score â‰¥ 0.7 relevance, each with a `relevance_score` field added  \n",
    "**LLM:** Uses `gpt-4o-mini` via OpenRouter for cost-effective reasoning\n",
    "\n",
    "The agent prompts the LLM to return a JSON object with a relevance score (0.0â€“1.0) and justification for each paper. Only papers meeting the threshold are added to `filtered_papers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Initialize the LLM for agents that need reasoning capabilities\n",
    "llm = ChatOpenAI(\n",
    "    model=\"openai/gpt-4o-mini\",\n",
    "    temperature=0,  # Deterministic output for consistent evaluations\n",
    "    openai_api_base=\"https://openrouter.ai/api/v1\",\n",
    "    openai_api_key=os.getenv(\"OPENROUTER_API_KEY\")\n",
    ")\n",
    "\n",
    "def filter_agent(state: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate the relevance of each paper to the research query using an LLM.\n",
    "    \n",
    "    Args:\n",
    "        state: Current graph state containing 'query' and 'papers'\n",
    "        \n",
    "    Returns:\n",
    "        Updated state with:\n",
    "        - 'filtered_papers': papers that scored >= 0.7\n",
    "        - 'all_evaluations': all papers with their scores and justifications (for debugging)\n",
    "    \"\"\"\n",
    "    query = state[\"query\"]\n",
    "    papers = state[\"papers\"]\n",
    "    \n",
    "    # SystemMessage: Defines the AI's role, behavior, and output format\n",
    "    # These are persistent instructions that apply to ALL evaluations\n",
    "    system_prompt = SystemMessage(content=\"\"\"You are an academic paper relevance evaluator.\n",
    "Your task is to assess how relevant a given paper is to a research query.\n",
    "Be objective and base your assessment on the paper's title, abstract, and publication date.\n",
    "\n",
    "When evaluating relevance, consider:\n",
    "- How directly the paper addresses the research query\n",
    "- The recency of the paper (more recent papers are preferred when content relevance is similar)\n",
    "\n",
    "You must respond with ONLY a valid JSON object in this exact format:\n",
    "{\"relevance_score\": 0.0, \"justification\": \"brief explanation\"}\n",
    "\n",
    "The relevance_score must be between 0.0 and 1.0 where:\n",
    "- 0.0-0.3: Not relevant (paper does not address the research query)\n",
    "- 0.4-0.6: Somewhat relevant (paper touches on related topics)\n",
    "- 0.7-1.0: Highly relevant (paper directly addresses the research query)\"\"\")\n",
    "    \n",
    "    filtered = []\n",
    "    all_evaluations = []  # Track ALL evaluations for debugging\n",
    "    \n",
    "    for paper in papers:\n",
    "        # HumanMessage: Contains ONLY the variable data for this specific evaluation\n",
    "        # No instructions here - just the inputs that change per paper\n",
    "        user_prompt = HumanMessage(content=f\"\"\"Research Query: {query}\n",
    "\n",
    "Paper Title: {paper['title']}\n",
    "\n",
    "Publication Date: {paper['published']}\n",
    "\n",
    "Abstract: {paper['abstract']}\"\"\")\n",
    "        \n",
    "        try:\n",
    "            # Pass both SystemMessage and HumanMessage to the LLM\n",
    "            # SystemMessage sets the behavior, HumanMessage provides the specific data\n",
    "            response = llm.invoke([system_prompt, user_prompt])\n",
    "            result = json.loads(response.content)\n",
    "            \n",
    "            score = result.get(\"relevance_score\", 0)\n",
    "            justification = result.get(\"justification\", \"\")\n",
    "            \n",
    "            # Record this evaluation (regardless of whether it passes)\n",
    "            evaluation = {\n",
    "                \"title\": paper[\"title\"],\n",
    "                \"published\": paper[\"published\"],\n",
    "                \"relevance_score\": score,\n",
    "                \"justification\": justification,\n",
    "                \"passed\": score >= 0.7\n",
    "            }\n",
    "            all_evaluations.append(evaluation)\n",
    "            \n",
    "            # Keep papers that meet the relevance threshold\n",
    "            if score >= 0.7:\n",
    "                paper_with_score = paper.copy()\n",
    "                paper_with_score[\"relevance_score\"] = score\n",
    "                paper_with_score[\"justification\"] = justification\n",
    "                filtered.append(paper_with_score)\n",
    "                \n",
    "        except (json.JSONDecodeError, Exception) as e:\n",
    "            # If parsing fails, record the error and skip this paper\n",
    "            all_evaluations.append({\n",
    "                \"title\": paper[\"title\"],\n",
    "                \"published\": paper[\"published\"],\n",
    "                \"relevance_score\": None,\n",
    "                \"justification\": f\"Error: {e}\",\n",
    "                \"passed\": False\n",
    "            })\n",
    "            print(f\"Filter Agent: Error evaluating '{paper['title'][:50]}...': {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Filter Agent: {len(filtered)}/{len(papers)} papers passed relevance threshold (>= 0.7)\")\n",
    "    \n",
    "    # Return both filtered papers and all evaluations\n",
    "    return {\n",
    "        \"filtered_papers\": filtered,\n",
    "        \"all_evaluations\": all_evaluations\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter Agent: 1/10 papers passed relevance threshold (>= 0.7)\n"
     ]
    }
   ],
   "source": [
    "filter_results = filter_agent(test_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filter_results[\"filtered_papers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Small LLMs Are Weak Tool Learners: A Multi-LLM Agent'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_results[\"filtered_papers\"][0][\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filter_results[\"all_evaluations\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARLBench: Flexible and Efficient Benchmarking for Hyperparameter Optimization in Reinforcement Learning 0.4 The paper discusses hyperparameter optimization in reinforcement learning, which is related to the broader topic of reinforcement learning but does not specifically address multi-LLM-agent reinforcement learning.\n",
      "Causal-Paced Deep Reinforcement Learning 0.4 The paper discusses reinforcement learning and curriculum learning, which are related to multi-agent reinforcement learning, but it does not specifically address multi-LLM agents or their integration.\n",
      "Exploring Hierarchy-Aware Inverse Reinforcement Learning 0.4 The paper discusses inverse reinforcement learning and hierarchical strategies, which are related to reinforcement learning concepts, but it does not specifically address multi-LLM-agent systems.\n",
      "Small LLMs Are Weak Tool Learners: A Multi-LLM Agent 0.8 The paper discusses a multi-LLM agent framework that addresses tool learning, which is relevant to multi-LLM-agent reinforcement learning, particularly in the context of task planning and execution.\n",
      "A Tutorial on Meta-Reinforcement Learning 0.4 The paper discusses meta-reinforcement learning, which is related to reinforcement learning but does not specifically address multi-LLM-agent systems.\n",
      "Anderson Acceleration for Reinforcement Learning 0.4 The paper discusses reinforcement learning and introduces a method that could be applied to it, but it does not specifically address multi-LLM-agent reinforcement learning.\n",
      "Stabilizing Extreme Q-learning by Maclaurin Expansion 0.4 The paper discusses reinforcement learning and introduces a method related to Q-learning, which is relevant to the broader topic of multi-agent reinforcement learning, but it does not specifically address multi-LLM agents or their integration.\n",
      "MERL: Multi-Head Reinforcement Learning 0.4 The paper discusses reinforcement learning and introduces a framework (MERL) that could relate to multi-agent systems, but it does not specifically address multi-LLM-agent reinforcement learning, making it only somewhat relevant.\n",
      "Context Engineering for Multi-Agent LLM Code Assistants Using Elicit, NotebookLM, ChatGPT, and Claude Code 0.6 The paper discusses multi-agent systems involving LLMs and their application in code generation, which relates to multi-LLM-agent reinforcement learning, but it does not explicitly focus on reinforcement learning aspects.\n",
      "Compression and Localization in Reinforcement Learning for ATARI Games 0.3 The paper discusses reinforcement learning and model compression, but it does not specifically address multi-LLM-agent reinforcement learning, making it only tangentially relevant.\n"
     ]
    }
   ],
   "source": [
    "for paper in filter_results[\"all_evaluations\"]:                                                     \n",
    "    print(paper[\"title\"], paper[\"relevance_score\"], paper[\"justification\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration constants for the Supervisor's decision logic\n",
    "MIN_RELEVANT_PAPERS = 3  # Minimum papers needed to consider search successful\n",
    "MAX_ITERATIONS = 3       # Maximum search attempts before giving up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervisor Agent\n",
    "\n",
    "The Supervisor Agent is the decision-maker that controls the workflow. After the Filter Agent evaluates papers, the Supervisor checks whether we have enough relevant results or need to refine the query and search again.\n",
    "\n",
    "**Input:** Takes `filtered_papers` and `iteration` from the state  \n",
    "**Output:** Returns a `decision` field: either `\"end\"` or `\"refine\"`  \n",
    "No LLM required, this is pure conditional logic, not reasoning.\n",
    "\n",
    "**Decision Logic:**\n",
    "1. If `filtered_papers` has â‰¥3 papers â†’ `\"end\"` (success)\n",
    "2. If `iteration` â‰¥ 3 â†’ `\"end\"` (max attempts reached, return what we have)\n",
    "3. Otherwise â†’ `\"refine\"` (try again with a refined query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supervisor_agent(state: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Decide whether to end the workflow or continue with query refinement.\n",
    "    \n",
    "    This agent uses simple conditional logic (no LLM) to make routing decisions\n",
    "    based on the current state of the search.\n",
    "    \n",
    "    Args:\n",
    "        state: Current graph state containing 'filtered_papers' and 'iteration'\n",
    "        \n",
    "    Returns:\n",
    "        Updated state with 'decision' field: \"end\" or \"refine\"\n",
    "    \"\"\"\n",
    "    filtered_papers = state[\"filtered_papers\"]\n",
    "    iteration = state[\"iteration\"]\n",
    "    \n",
    "    num_relevant = len(filtered_papers)\n",
    "    stop_reason = \"\"\n",
    "    \n",
    "    # Decision logic\n",
    "    if num_relevant >= MIN_RELEVANT_PAPERS:\n",
    "        # Success: we have enough relevant papers\n",
    "        decision = \"end\"\n",
    "        stop_reason = f\"Success: Found {num_relevant} relevant papers (>= {MIN_RELEVANT_PAPERS} required)\"\n",
    "    elif iteration >= MAX_ITERATIONS:\n",
    "        # Max attempts reached: return what we have\n",
    "        decision = \"end\"\n",
    "        stop_reason = f\"Max iterations ({MAX_ITERATIONS}) reached with only {num_relevant} relevant papers\"\n",
    "    else:\n",
    "        # Need more results: refine query and try again\n",
    "        decision = \"refine\"\n",
    "    \n",
    "    print(f\"Supervisor Agent: {decision.upper()}\" + (f\" - {stop_reason}\" if stop_reason else f\" - Refining query (iteration {iteration + 1})\"))\n",
    "    \n",
    "    return {\"decision\": decision}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Refinement Agent\n",
    "\n",
    "The Query Refinement Agent improves the search query when the current results are insufficient. It **uses an LLM** to reason about why the previous query didn't yield enough relevant papers and how to improve it.\n",
    "\n",
    "**Input:** Takes `query`, `all_evaluations`, and `iteration` from the state  \n",
    "**Output:** Returns an updated `query` string and increments `iteration`  \n",
    "**LLM:** Uses `gpt-4o-mini` via OpenRouter to analyze feedback and generate better queries\n",
    "\n",
    "The agent examines the evaluation feedback (why papers were rejected) and uses that insight to craft a more targeted query. For example, if many papers were rejected for being too theoretical, it might add terms like \"applied\" or \"practical\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_refinement_agent(state: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Refine the search query based on feedback from previous evaluations.\n",
    "    \n",
    "    Analyzes why papers were rejected and generates an improved query\n",
    "    that is more likely to find relevant results.\n",
    "    \n",
    "    Args:\n",
    "        state: Current graph state containing 'query', 'all_evaluations', and 'iteration'\n",
    "        \n",
    "    Returns:\n",
    "        Updated state with:\n",
    "        - 'query': refined search query\n",
    "        - 'iteration': incremented iteration count\n",
    "    \"\"\"\n",
    "    current_query = state[\"query\"]\n",
    "    all_evaluations = state[\"all_evaluations\"]\n",
    "    iteration = state[\"iteration\"]\n",
    "    \n",
    "    # Format evaluation feedback for the LLM\n",
    "    feedback_lines = []\n",
    "    for eval in all_evaluations:\n",
    "        status = \"PASSED\" if eval[\"passed\"] else \"REJECTED\"\n",
    "        feedback_lines.append(\n",
    "            f\"- [{status}] \\\"{eval['title']}\\\" (score: {eval['relevance_score']}) - {eval['justification']}\"\n",
    "        )\n",
    "    feedback_summary = \"\\n\".join(feedback_lines)\n",
    "    \n",
    "    # SystemMessage: Defines the AI's role and output format\n",
    "    system_prompt = SystemMessage(content=\"\"\"You are a search query optimization expert.\n",
    "    Your task is to refine academic search queries based on feedback from previous search results.\n",
    "\n",
    "    Analyze why papers were rejected and craft a more targeted query that will find more relevant results.\n",
    "    Consider:\n",
    "    - Adding specific technical terms that were missing\n",
    "    - Removing overly broad or ambiguous terms\n",
    "    - Including synonyms or related concepts\n",
    "    - Narrowing the scope if results were too general\n",
    "\n",
    "    You must respond with ONLY the refined query string, nothing else.\n",
    "    Do not include quotes around the query. Just output the query text directly.\"\"\")\n",
    "    \n",
    "    # HumanMessage: Contains the specific data for this refinement\n",
    "    user_prompt = HumanMessage(content=f\"\"\"Current Query: {current_query}\n",
    "\n",
    "    Iteration: {iteration + 1}\n",
    "\n",
    "    Previous Search Results Feedback:\n",
    "    {feedback_summary}\n",
    "\n",
    "    Based on this feedback, generate an improved search query that will find more relevant papers.\"\"\")\n",
    "    \n",
    "    try:\n",
    "        response = llm.invoke([system_prompt, user_prompt])\n",
    "        refined_query = response.content.strip()\n",
    "        \n",
    "        # Clean up the query (remove quotes if LLM added them)\n",
    "        refined_query = refined_query.strip('\"\\'')\n",
    "        \n",
    "        print(f\"Query Refinement Agent: '{current_query}' â†’ '{refined_query}'\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        # If refinement fails, add \"survey\" to find overview papers\n",
    "        refined_query = f\"{current_query} survey\"\n",
    "        print(f\"Query Refinement Agent: Error ({e}), using fallback: '{refined_query}'\")\n",
    "    \n",
    "    return {\n",
    "        \"query\": refined_query,\n",
    "        \"iteration\": iteration + 1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state before refinement:\n",
      "  query: 'multi-llm-agent reinforcement learning'\n",
      "  iteration: 0\n",
      "  all_evaluations: 10 items\n",
      "\n",
      "Query Refinement Agent: 'multi-llm-agent reinforcement learning' â†’ 'multi-llm-agent reinforcement learning framework tool learning task planning execution'\n",
      "\n",
      "Refinement result:\n",
      "  new query: 'multi-llm-agent reinforcement learning framework tool learning task planning execution'\n",
      "  new iteration: 1\n"
     ]
    }
   ],
   "source": [
    "# Test the Query Refinement Agent\n",
    "# The test_state already has all_evaluations from the filter agent test\n",
    "\n",
    "print(f\"Current state before refinement:\")\n",
    "print(f\"  query: '{test_state['query']}'\")\n",
    "print(f\"  iteration: {test_state['iteration']}\")\n",
    "print(f\"  all_evaluations: {len(test_state['all_evaluations'])} items\")\n",
    "print()\n",
    "\n",
    "# Test query refinement agent\n",
    "refinement_result = query_refinement_agent(test_state)\n",
    "\n",
    "print()\n",
    "print(f\"Refinement result:\")\n",
    "print(f\"  new query: '{refinement_result['query']}'\")\n",
    "print(f\"  new iteration: {refinement_result['iteration']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Graph\n",
    "\n",
    "Now that we have all four agents defined, we wire them together into a LangGraph `StateGraph`. The graph defines:\n",
    "\n",
    "1. **Nodes** â€” Each agent function becomes a node in the graph\n",
    "2. **Edges** â€” Define the flow between nodes (which agent runs after which)\n",
    "3. **Conditional Edges** â€” Allow dynamic routing based on state (the Supervisor's decision)\n",
    "\n",
    "Recall, the workflow follows this pattern:  \n",
    "<img src=\"https://raw.githubusercontent.com/tligorio/multiagent_langgraph_tutorial/main/images/multiagent.png\" alt=\"Multiagent System - web\" width=\"50%\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph compiled successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create the StateGraph with our state schema\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes â€” each agent function becomes a node\n",
    "graph.add_node(\"search_agent\", search_agent)\n",
    "graph.add_node(\"filter_agent\", filter_agent)\n",
    "graph.add_node(\"supervisor_agent\", supervisor_agent)\n",
    "graph.add_node(\"query_refinement_agent\", query_refinement_agent)\n",
    "\n",
    "# Add edges â€” define the linear flow\n",
    "graph.add_edge(START, \"search_agent\")           # Entry point\n",
    "graph.add_edge(\"search_agent\", \"filter_agent\")  # Search â†’ Filter\n",
    "graph.add_edge(\"filter_agent\", \"supervisor_agent\")  # Filter â†’ Supervisor\n",
    "\n",
    "# Add conditional edge â€” Supervisor decides next step based on 'decision' field\n",
    "def route_supervisor(state: dict) -> str:\n",
    "    \"\"\"Route based on supervisor's decision.\"\"\"\n",
    "    if state[\"decision\"] == \"end\":\n",
    "        return END\n",
    "    else:\n",
    "        return \"query_refinement_agent\"\n",
    "\n",
    "graph.add_conditional_edges(\n",
    "    \"supervisor_agent\",\n",
    "    route_supervisor,\n",
    "    {END: END, \"query_refinement_agent\": \"query_refinement_agent\"}\n",
    ")\n",
    "\n",
    "# Query refinement loops back to search\n",
    "graph.add_edge(\"query_refinement_agent\", \"search_agent\")\n",
    "\n",
    "# Compile the graph into a runnable workflow\n",
    "workflow = graph.compile()\n",
    "\n",
    "print(\"Graph compiled successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Workflow\n",
    "\n",
    "Now we can run the complete workflow by invoking the compiled graph with an initial state. The graph will:\n",
    "\n",
    "1. Start with the search agent\n",
    "2. Filter results for relevance\n",
    "3. Check if we have enough papers (Supervisor)\n",
    "4. If not, refine the query and repeat\n",
    "5. Continue until we have â‰¥3 relevant papers or hit the max iteration limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting workflow with query: 'multi-llm-agent reinforcement learning'\n",
      "============================================================\n",
      "Search Agent: Found 10 papers for query 'multi-llm-agent reinforcement learning'\n",
      "Filter Agent: 1/10 papers passed relevance threshold (>= 0.7)\n",
      "Supervisor Agent: REFINE - Refining query (iteration 1)\n",
      "Query Refinement Agent: 'multi-llm-agent reinforcement learning' â†’ 'multi-llm-agent reinforcement learning collaboration tool learning'\n",
      "Search Agent: Found 10 papers for query 'multi-llm-agent reinforcement learning collaboration tool learning'\n",
      "Filter Agent: 0/10 papers passed relevance threshold (>= 0.7)\n",
      "Supervisor Agent: REFINE - Refining query (iteration 2)\n",
      "Query Refinement Agent: 'multi-llm-agent reinforcement learning collaboration tool learning' â†’ 'multi-agent reinforcement learning collaboration tools for large language models'\n",
      "Search Agent: Found 10 papers for query 'multi-agent reinforcement learning collaboration tools for large language models'\n",
      "Filter Agent: 3/10 papers passed relevance threshold (>= 0.7)\n",
      "Supervisor Agent: END - Success: Found 4 relevant papers (>= 3 required)\n",
      "============================================================\n",
      "\n",
      "Workflow complete!\n",
      "Result: Success: Found 4 relevant papers\n",
      "Final query: 'multi-agent reinforcement learning collaboration tools for large language models'\n",
      "Total iterations: 2\n",
      "Relevant papers found: 4\n"
     ]
    }
   ],
   "source": [
    "# Define the initial state with our research query\n",
    "initial_state = {\n",
    "    \"query\": \"multi-llm-agent reinforcement learning\",\n",
    "    \"papers\": [],\n",
    "    \"filtered_papers\": [],\n",
    "    \"all_evaluations\": [],\n",
    "    \"iteration\": 0,\n",
    "    \"decision\": \"\"\n",
    "}\n",
    "\n",
    "print(f\"Starting workflow with query: '{initial_state['query']}'\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run the workflow\n",
    "final_state = workflow.invoke(initial_state)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nWorkflow complete!\")\n",
    "\n",
    "# Determine stop reason from final state values\n",
    "num_found = len(final_state['filtered_papers'])\n",
    "if num_found >= MIN_RELEVANT_PAPERS:\n",
    "    stop_reason = f\"Success: Found {num_found} relevant papers\"\n",
    "else:\n",
    "    stop_reason = f\"Max iterations reached with only {num_found} relevant papers\"\n",
    "\n",
    "print(f\"Result: {stop_reason}\")\n",
    "print(f\"Final query: '{final_state['query']}'\")\n",
    "print(f\"Total iterations: {final_state['iteration']}\")\n",
    "print(f\"Relevant papers found: {num_found}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant Papers Found:\n",
      "------------------------------------------------------------\n",
      "\n",
      "1. Small LLMs Are Weak Tool Learners: A Multi-LLM Agent\n",
      "   Published: 2024-01-14\n",
      "   Relevance: 0.8\n",
      "   URL: http://arxiv.org/abs/2401.07324v3\n",
      "\n",
      "2. Reinforcement Learning Meets Large Language Models: A Survey of Advancements and Applications Across the LLM Lifecycle\n",
      "   Published: 2025-09-20\n",
      "   Relevance: 0.7\n",
      "   URL: http://arxiv.org/abs/2509.16679v1\n",
      "\n",
      "3. Hierarchical Multi-agent Large Language Model Reasoning for Autonomous Functional Materials Discovery\n",
      "   Published: 2025-12-15\n",
      "   Relevance: 0.7\n",
      "   URL: http://arxiv.org/abs/2512.13930v1\n",
      "\n",
      "4. Towards Effective GenAI Multi-Agent Collaboration: Design and Evaluation for Enterprise Applications\n",
      "   Published: 2024-12-06\n",
      "   Relevance: 0.8\n",
      "   URL: http://arxiv.org/abs/2412.05449v1\n"
     ]
    }
   ],
   "source": [
    "# Display the relevant papers found\n",
    "print(\"Relevant Papers Found:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i, paper in enumerate(final_state[\"filtered_papers\"], 1):\n",
    "    print(f\"\\n{i}. {paper['title']}\")\n",
    "    print(f\"   Published: {paper['published']}\")\n",
    "    print(f\"   Relevance: {paper['relevance_score']}\")\n",
    "    print(f\"   URL: {paper['url']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements\n",
    "\n",
    "1. **Duplicate papers** â€” Papers may appear more than once in the final results. This happens because `filtered_papers` accumulates across iterations â€” if the same paper is found in multiple searches and passes the relevance threshold each time, it gets added again.\n",
    "\n",
    "   *Exercise for the reader:* Modify the workflow to prevent duplicate papers. Consider deduplicating by paper URL or title in the `filter_agent`, keeping track of already-seen paper IDs in the state, or deduplicating at the end before displaying results.\n",
    "\n",
    "2. **Multiple search sources** â€” More search agents may be added to use different academic paper sources: Semantic Scholar, PubMed, OpenAlex, CrossRef, IEEE Xplore, ACM Digital Library, and others.\n",
    "\n",
    "   *Exercise for the reader:* Implement additional agents for different searches (e.g., `semantic_scholar_agent`, `pubmed_agent`, etc.) and modify the graph to run multiple search agents in parallel. Consider how the state schema should change, whether the filter agent should treat sources differently, and how to handle cross-database duplicates. Should all sources always be searched? What logic determines which sources to use â€” the research domain, the query keywords, the iteration number? Or should an LLM agent decide?\n",
    "\n",
    "3. **Error handling and retries** â€” The workflow currently assumes API calls succeed. What happens if arXiv is down, rate limits us mid-workflow, or returns malformed data?\n",
    "\n",
    "   *Exercise for the reader:* Add try/except blocks, exponential backoff, and graceful degradation so the workflow can recover from transient failures.\n",
    "\n",
    "4. **Citation following** â€” Once relevant papers are found, expanding the search to include papers they cite (references) or papers that cite them (citations) could surface important related work.\n",
    "\n",
    "   *Exercise for the reader:* Implement a `citation_agent` that takes the filtered papers and queries a citation API (e.g., Semantic Scholar) to find connected papers. How should this agent integrate into the existing graph?\n",
    "\n",
    "5. **Summarization agent** â€” After finding relevant papers, a summarization step could help users quickly understand the landscape.\n",
    "\n",
    "   *Exercise for the reader:* Add a final agent that synthesizes the findings: generate a research summary, identify common themes across papers, or produce a literature review outline.\n",
    "\n",
    "6. **User feedback loop** â€” The current workflow is fully automated. Allowing user input during execution could improve results.\n",
    "\n",
    "   *Exercise for the reader:* Modify the workflow to pause and ask the user to mark papers as relevant or irrelevant. Use this feedback to adjust subsequent searches or filter criteria.\n",
    "\n",
    "7. **Export functionality** â€” Researchers need results in formats compatible with their tools.\n",
    "\n",
    "   *Exercise for the reader:* Add an export step that outputs results to BibTeX for LaTeX, CSV for spreadsheets, or direct integration with reference managers like Zotero or Mendeley.\n",
    "\n",
    "8. **Checkpointing** â€” Long-running workflows may be interrupted. LangGraph supports persistence and checkpointing.\n",
    "\n",
    "   *Exercise for the reader:* Configure the workflow to save state periodically so it can be resumed if interrupted. See the LangGraph documentation on persistence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multiagent_langgraph_tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
